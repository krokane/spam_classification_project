{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment A, SVM"
      ],
      "metadata": {
        "id": "qwbHXOYgnWVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH2rjNlumrlY",
        "outputId": "5fe1887c-e71e-4d49-dfe2-cc46a2f033f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 500\n",
            "Accuracy: 87.5%\n",
            "Precision: 86.17%\n",
            "Recall: 89.33%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 1000\n",
            "Accuracy: 91.4%\n",
            "Precision: 91.9%\n",
            "Recall: 90.8%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 2000\n",
            "Accuracy: 92.93%\n",
            "Precision: 92.58999999999999%\n",
            "Recall: 93.33%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 3000\n",
            "Accuracy: 93.7%\n",
            "Precision: 93.21000000000001%\n",
            "Recall: 94.27%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 4000\n",
            "Accuracy: 94.17%\n",
            "Precision: 93.5%\n",
            "Recall: 94.93%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 5000\n",
            "Accuracy: 92.47%\n",
            "Precision: 94.61%\n",
            "Recall: 90.07%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 7500\n",
            "Accuracy: 95.5%\n",
            "Precision: 94.28999999999999%\n",
            "Recall: 96.87%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 10000\n",
            "Accuracy: 95.7%\n",
            "Precision: 94.31%\n",
            "Recall: 97.27%\n",
            "\n",
            "Sample Size: 20000\n",
            "Accuracy: 96.87%\n",
            "Precision: 95.59%\n",
            "Recall: 98.27%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import sklearn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "train = pd.read_csv('train_data.csv')\n",
        "test = pd.read_csv('test_data.csv')\n",
        "\n",
        "sample_list = [500,1000,2000,3000,4000,5000,7500,10000,len(train)]\n",
        "\n",
        "#token and data cleaning function\n",
        "def clean_text(text):\n",
        "    #removes body prefix\n",
        "    if type(text) != str:\n",
        "        return 'gibberishnonsensenothingeverseenbefore'\n",
        "    #replaces b's at beginning of body paragraphs\n",
        "    if text[:2] == \"b'\" or text[:2] == 'b\"':\n",
        "        text = text[2:]\n",
        "        text = text[:-1]\n",
        "    #replaces digit with digit tags\n",
        "    text = re.sub(r\"\\d\", \"<digit>\", text)\n",
        "    #removes new line symbols\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "    #removes periods, commas, dashes, apostrophes, quotes, and new lines\n",
        "    text = re.sub(r\"[.,:'/\\-(){}[\\]\\\"]\", \"\", text)\n",
        "    text = re.sub(r'\"', \"\", text)\n",
        "    #removes case\n",
        "    text = text.lower()\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "for i in range(len(sample_list)):\n",
        "    train_sample = train.sample(n=sample_list[i], random_state=1738)\n",
        "\n",
        "    #deploy function, add to DFs\n",
        "    train_cleaned = []\n",
        "    for j in range(len(train_sample)):\n",
        "        train_cleaned.append(clean_text(train_sample.iloc[j]['body']))\n",
        "    test_cleaned = []\n",
        "    for k in range(len(test)):\n",
        "        test_cleaned.append(clean_text(test.iloc[k]['body']))\n",
        "    train_sample['clean'] = train_cleaned\n",
        "    test['clean'] = test_cleaned\n",
        "\n",
        "\n",
        "    #vectorize data\n",
        "    vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "    train_X = vectorizer.fit_transform(train_sample['clean'])\n",
        "    test_X = vectorizer.transform(test['clean'])\n",
        "\n",
        "    tr_binary_labels = []\n",
        "    te_binary_labels = []\n",
        "    for l in range(len(train_sample)):\n",
        "        if train_sample.iloc[l]['label'] == 'ham':\n",
        "            tr_binary_labels.append(0)\n",
        "        else: tr_binary_labels.append(1)\n",
        "    for m in range(len(test)):\n",
        "        if test.iloc[m]['label'] == 'ham':\n",
        "            te_binary_labels.append(0)\n",
        "        else: te_binary_labels.append(1)\n",
        "    train_Y = tr_binary_labels\n",
        "    test_Y = te_binary_labels\n",
        "\n",
        "\n",
        "    #Naive Bayes Model\n",
        "    model = LinearSVC()\n",
        "\n",
        "    fit = model.fit(train_X, train_Y)\n",
        "\n",
        "    predictions = fit.predict(test_X)\n",
        "\n",
        "    accuracy = accuracy_score(test_Y, predictions)\n",
        "    precision = precision_score(test_Y, predictions)\n",
        "    recall = recall_score(test_Y, predictions)\n",
        "    print(f'Sample Size: {sample_list[i]}')\n",
        "    print(f'Accuracy: {100*round(accuracy,4)}%')\n",
        "    print(f'Precision: {100*round(precision,4)}%')\n",
        "    print(f'Recall: {100*round(recall,4)}%\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment A, Naive Bayes"
      ],
      "metadata": {
        "id": "WQbPrKcOngFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import sklearn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "train = pd.read_csv('train_data.csv')\n",
        "test = pd.read_csv('test_data.csv')\n",
        "\n",
        "sample_list = [500,1000,2000,3000,4000,5000,7500,10000,len(train)]\n",
        "\n",
        "#token and data cleaning function\n",
        "def clean_text(text):\n",
        "    #removes body prefix\n",
        "    if type(text) != str:\n",
        "        return 'gibberishnonsensenothingeverseenbefore'\n",
        "    #replaces b's at beginning of body paragraphs\n",
        "    if text[:2] == \"b'\" or text[:2] == 'b\"':\n",
        "        text = text[2:]\n",
        "        text = text[:-1]\n",
        "    #replaces digit with digit tags\n",
        "    text = re.sub(r\"\\d\", \"<digit>\", text)\n",
        "    #removes new line symbols\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "    #removes periods, commas, dashes, apostrophes, quotes, and new lines\n",
        "    text = re.sub(r\"[.,:'/\\-(){}[\\]\\\"]\", \"\", text)\n",
        "    text = re.sub(r'\"', \"\", text)\n",
        "    #removes case\n",
        "    text = text.lower()\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "for i in range(len(sample_list)):\n",
        "    train_sample = train.sample(n=sample_list[i], random_state=1738)\n",
        "\n",
        "    #deploy function, add to DFs\n",
        "    train_cleaned = []\n",
        "    for j in range(len(train_sample)):\n",
        "        train_cleaned.append(clean_text(train_sample.iloc[j]['body']))\n",
        "    test_cleaned = []\n",
        "    for k in range(len(test)):\n",
        "        test_cleaned.append(clean_text(test.iloc[k]['body']))\n",
        "    train_sample['clean'] = train_cleaned\n",
        "    test['clean'] = test_cleaned\n",
        "\n",
        "\n",
        "    #vectorize data\n",
        "    vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "    train_X = vectorizer.fit_transform(train_sample['clean'])\n",
        "    test_X = vectorizer.transform(test['clean'])\n",
        "\n",
        "    tr_binary_labels = []\n",
        "    te_binary_labels = []\n",
        "    for l in range(len(train_sample)):\n",
        "        if train_sample.iloc[l]['label'] == 'ham':\n",
        "            tr_binary_labels.append(0)\n",
        "        else: tr_binary_labels.append(1)\n",
        "    for m in range(len(test)):\n",
        "        if test.iloc[m]['label'] == 'ham':\n",
        "            te_binary_labels.append(0)\n",
        "        else: te_binary_labels.append(1)\n",
        "    train_Y = tr_binary_labels\n",
        "    test_Y = te_binary_labels\n",
        "\n",
        "\n",
        "    #Naive Bayes Model\n",
        "    model = MultinomialNB()\n",
        "\n",
        "    fit = model.fit(train_X, train_Y)\n",
        "\n",
        "    predictions = fit.predict(test_X)\n",
        "\n",
        "    accuracy = accuracy_score(test_Y, predictions)\n",
        "    precision = precision_score(test_Y, predictions)\n",
        "    recall = recall_score(test_Y, predictions)\n",
        "    print(f'Sample Size: {sample_list[i]}')\n",
        "    print(f'Accuracy: {100*round(accuracy,4)}%')\n",
        "    print(f'Precision: {100*round(precision,4)}%')\n",
        "    print(f'Recall: {100*round(recall,4)}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEKld44cneS4",
        "outputId": "017dc697-9348-4975-c60b-90c404d3c96d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Size: 500\n",
            "Accuracy: 91.13%\n",
            "Precision: 94.32000000000001%\n",
            "Recall: 87.53%\n",
            "\n",
            "Sample Size: 1000\n",
            "Accuracy: 92.53%\n",
            "Precision: 95.38%\n",
            "Recall: 89.4%\n",
            "\n",
            "Sample Size: 2000\n",
            "Accuracy: 94.83%\n",
            "Precision: 95.78%\n",
            "Recall: 93.8%\n",
            "\n",
            "Sample Size: 3000\n",
            "Accuracy: 95.03%\n",
            "Precision: 95.67%\n",
            "Recall: 94.33%\n",
            "\n",
            "Sample Size: 4000\n",
            "Accuracy: 94.8%\n",
            "Precision: 96.6%\n",
            "Recall: 92.86999999999999%\n",
            "\n",
            "Sample Size: 5000\n",
            "Accuracy: 95.07%\n",
            "Precision: 96.81%\n",
            "Recall: 93.2%\n",
            "\n",
            "Sample Size: 7500\n",
            "Accuracy: 95.17%\n",
            "Precision: 96.63000000000001%\n",
            "Recall: 93.60000000000001%\n",
            "\n",
            "Sample Size: 10000\n",
            "Accuracy: 95.47%\n",
            "Precision: 96.71%\n",
            "Recall: 94.13%\n",
            "\n",
            "Sample Size: 20000\n",
            "Accuracy: 96.03%\n",
            "Precision: 96.81%\n",
            "Recall: 95.19999999999999%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment B, SVM"
      ],
      "metadata": {
        "id": "MIPSloGunroi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import sklearn\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "train = pd.read_csv('train_data.csv')\n",
        "test = pd.read_csv('test_data.csv')\n",
        "\n",
        "#token and data cleaning function\n",
        "def clean_text(text):\n",
        "    #removes body prefix\n",
        "    if type(text) != str:\n",
        "        return 'gibberishnonsensenothingeverseenbefore'\n",
        "    #replaces b's at beginning of body paragraphs\n",
        "    if text[:2] == \"b'\" or text[:2] == 'b\"':\n",
        "        text = text[2:]\n",
        "        text = text[:-1]\n",
        "    #replaces digit with digit tags\n",
        "    text = re.sub(r\"\\d\", \"<digit>\", text)\n",
        "    #removes new line symbols\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "    #removes periods, commas, dashes, apostrophes, quotes, and new lines\n",
        "    text = re.sub(r\"[.,:'/\\-(){}[\\]\\\"]\", \"\", text)\n",
        "    text = re.sub(r'\"', \"\", text)\n",
        "    #removes case\n",
        "    text = text.lower()\n",
        "    return text.split()\n",
        "\n",
        "#deploy function, add to DFs\n",
        "train_cleaned = []\n",
        "for j in range(len(train)):\n",
        "    train_cleaned.append(clean_text(train.iloc[j]['body']))\n",
        "test_cleaned = []\n",
        "for k in range(len(test)):\n",
        "    test_cleaned.append(clean_text(test.iloc[k]['body']))\n",
        "train['clean'] = train_cleaned\n",
        "test['clean'] = test_cleaned\n",
        "\n",
        "#vectorize data\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "train_X = vectorizer.fit_transform(train['clean'])\n",
        "\n",
        "tr_binary_labels = []\n",
        "te_binary_labels = []\n",
        "for l in range(len(train)):\n",
        "    if train.iloc[l]['label'] == 'ham':\n",
        "        tr_binary_labels.append(0)\n",
        "    else: tr_binary_labels.append(1)\n",
        "for m in range(len(test)):\n",
        "    if test.iloc[m]['label'] == 'ham':\n",
        "        te_binary_labels.append(0)\n",
        "    else: te_binary_labels.append(1)\n",
        "train_Y = tr_binary_labels\n",
        "test_Y = te_binary_labels\n",
        "\n",
        "#Naive Bayes Model\n",
        "model = LinearSVC()\n",
        "fit = model.fit(train_X, train_Y)\n",
        "\n",
        "imp_words_list = [1,3,5,10,15,20,25,30,40,50,100,200]\n",
        "\n",
        "for o in range(len(imp_words_list)):\n",
        "    #pull top X words\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    importance = np.abs(model.coef_[0])\n",
        "    vocab = dict(zip(words, importance))\n",
        "\n",
        "    def get_important_words(input_list, n):\n",
        "        input_vocab = {word: vocab.get(word,0) for word in input_list}\n",
        "        sorted_vocab = sorted(input_vocab.items(), key=lambda x:x[1], reverse=True)[:n]\n",
        "        return [word for word, _ in sorted_vocab]\n",
        "\n",
        "    test_top_n = []\n",
        "    for i in range(len(test['clean'])):\n",
        "        test_top_n.append(get_important_words(test['clean'][i],imp_words_list[o]))\n",
        "\n",
        "    test_X = vectorizer.transform(test_top_n)\n",
        "\n",
        "    #make predictions with X important words\n",
        "    predictions = fit.predict(test_X)\n",
        "\n",
        "    accuracy = accuracy_score(test_Y, predictions)\n",
        "    precision = precision_score(test_Y, predictions)\n",
        "    recall = recall_score(test_Y, predictions)\n",
        "\n",
        "    print(f'Important Words: {imp_words_list[o]}')\n",
        "    print(f'Accuracy: {100*round(accuracy,4)}%')\n",
        "    print(f'Precision: {100*round(precision,4)}%')\n",
        "    print(f'Recall: {100*round(recall,4)}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Olv-FaPwn2HX",
        "outputId": "b928f7bc-e96e-41b7-e568-489c92f9de65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Words: 1\n",
            "Accuracy: 77.33%\n",
            "Precision: 75.37%\n",
            "Recall: 81.2%\n",
            "\n",
            "Important Words: 3\n",
            "Accuracy: 84.2%\n",
            "Precision: 85.97%\n",
            "Recall: 81.73%\n",
            "\n",
            "Important Words: 5\n",
            "Accuracy: 85.8%\n",
            "Precision: 88.91%\n",
            "Recall: 81.8%\n",
            "\n",
            "Important Words: 10\n",
            "Accuracy: 87.7%\n",
            "Precision: 90.08%\n",
            "Recall: 84.73%\n",
            "\n",
            "Important Words: 15\n",
            "Accuracy: 89.57000000000001%\n",
            "Precision: 91.77%\n",
            "Recall: 86.92999999999999%\n",
            "\n",
            "Important Words: 20\n",
            "Accuracy: 90.5%\n",
            "Precision: 92.57%\n",
            "Recall: 88.07000000000001%\n",
            "\n",
            "Important Words: 25\n",
            "Accuracy: 91.23%\n",
            "Precision: 93.04%\n",
            "Recall: 89.13%\n",
            "\n",
            "Important Words: 30\n",
            "Accuracy: 92.13%\n",
            "Precision: 93.47%\n",
            "Recall: 90.60000000000001%\n",
            "\n",
            "Important Words: 40\n",
            "Accuracy: 92.9%\n",
            "Precision: 93.92%\n",
            "Recall: 91.73%\n",
            "\n",
            "Important Words: 50\n",
            "Accuracy: 93.93%\n",
            "Precision: 93.99%\n",
            "Recall: 93.87%\n",
            "\n",
            "Important Words: 100\n",
            "Accuracy: 95.27%\n",
            "Precision: 94.15%\n",
            "Recall: 96.53%\n",
            "\n",
            "Important Words: 200\n",
            "Accuracy: 95.8%\n",
            "Precision: 94.61%\n",
            "Recall: 97.13000000000001%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment B, Naive Bayes"
      ],
      "metadata": {
        "id": "fL066bpLnuH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import sklearn\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "train = pd.read_csv('train_data.csv')\n",
        "test = pd.read_csv('test_data.csv')\n",
        "\n",
        "#token and data cleaning function\n",
        "def clean_text(text):\n",
        "    #removes body prefix\n",
        "    if type(text) != str:\n",
        "        return 'gibberishnonsensenothingeverseenbefore'\n",
        "    #replaces b's at beginning of body paragraphs\n",
        "    if text[:2] == \"b'\" or text[:2] == 'b\"':\n",
        "        text = text[2:]\n",
        "        text = text[:-1]\n",
        "    #replaces digit with digit tags\n",
        "    text = re.sub(r\"\\d\", \"<digit>\", text)\n",
        "    #removes new line symbols\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "    #removes periods, commas, dashes, apostrophes, quotes, and new lines\n",
        "    text = re.sub(r\"[.,:'/\\-(){}[\\]\\\"]\", \"\", text)\n",
        "    text = re.sub(r'\"', \"\", text)\n",
        "    #removes case\n",
        "    text = text.lower()\n",
        "    return text.split()\n",
        "\n",
        "#deploy function, add to DFs\n",
        "train_cleaned = []\n",
        "for j in range(len(train)):\n",
        "    train_cleaned.append(clean_text(train.iloc[j]['body']))\n",
        "test_cleaned = []\n",
        "for k in range(len(test)):\n",
        "    test_cleaned.append(clean_text(test.iloc[k]['body']))\n",
        "train['clean'] = train_cleaned\n",
        "test['clean'] = test_cleaned\n",
        "\n",
        "#vectorize data\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "train_X = vectorizer.fit_transform(train['clean'])\n",
        "\n",
        "tr_binary_labels = []\n",
        "te_binary_labels = []\n",
        "for l in range(len(train)):\n",
        "    if train.iloc[l]['label'] == 'ham':\n",
        "        tr_binary_labels.append(0)\n",
        "    else: tr_binary_labels.append(1)\n",
        "for m in range(len(test)):\n",
        "    if test.iloc[m]['label'] == 'ham':\n",
        "        te_binary_labels.append(0)\n",
        "    else: te_binary_labels.append(1)\n",
        "train_Y = tr_binary_labels\n",
        "test_Y = te_binary_labels\n",
        "\n",
        "#Naive Bayes Model\n",
        "model = MultinomialNB()\n",
        "fit = model.fit(train_X, train_Y)\n",
        "\n",
        "imp_words_list = [1,3,5,10,15,20,25,30,40,50,100,200]\n",
        "\n",
        "for o in range(len(imp_words_list)):\n",
        "    #pull top X words\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    importance = np.abs(model.feature_log_prob_[1]-model.feature_log_prob_[0])\n",
        "    vocab = dict(zip(words, importance))\n",
        "\n",
        "    def get_important_words(input_list, n):\n",
        "        input_vocab = {word: vocab.get(word,0) for word in input_list}\n",
        "        sorted_vocab = sorted(input_vocab.items(), key=lambda x:x[1], reverse=True)[:n]\n",
        "        return [word for word, _ in sorted_vocab]\n",
        "\n",
        "    test_top_n = []\n",
        "    for i in range(len(test['clean'])):\n",
        "        test_top_n.append(get_important_words(test['clean'][i],imp_words_list[o]))\n",
        "\n",
        "    test_X = vectorizer.transform(test_top_n)\n",
        "\n",
        "    #make predictions with X important words\n",
        "    predictions = fit.predict(test_X)\n",
        "\n",
        "    accuracy = accuracy_score(test_Y, predictions)\n",
        "    precision = precision_score(test_Y, predictions)\n",
        "    recall = recall_score(test_Y, predictions)\n",
        "\n",
        "    print(f'Important Words: {imp_words_list[o]}')\n",
        "    print(f'Accuracy: {100*round(accuracy,4)}%')\n",
        "    print(f'Precision: {100*round(precision,4)}%')\n",
        "    print(f'Recall: {100*round(recall,4)}%\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQouXRDen2dx",
        "outputId": "0515206c-945c-4bbd-8c4d-7cd06eb4f718"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Words: 1\n",
            "Accuracy: 93.87%\n",
            "Precision: 96.87%\n",
            "Recall: 90.67%\n",
            "\n",
            "Important Words: 3\n",
            "Accuracy: 95.39999999999999%\n",
            "Precision: 97.16%\n",
            "Recall: 93.53%\n",
            "\n",
            "Important Words: 5\n",
            "Accuracy: 95.73%\n",
            "Precision: 97.44%\n",
            "Recall: 93.93%\n",
            "\n",
            "Important Words: 10\n",
            "Accuracy: 95.77%\n",
            "Precision: 97.25%\n",
            "Recall: 94.19999999999999%\n",
            "\n",
            "Important Words: 15\n",
            "Accuracy: 96.37%\n",
            "Precision: 97.35000000000001%\n",
            "Recall: 95.33%\n",
            "\n",
            "Important Words: 20\n",
            "Accuracy: 96.23%\n",
            "Precision: 97.21%\n",
            "Recall: 95.19999999999999%\n",
            "\n",
            "Important Words: 25\n",
            "Accuracy: 96.37%\n",
            "Precision: 97.35000000000001%\n",
            "Recall: 95.33%\n",
            "\n",
            "Important Words: 30\n",
            "Accuracy: 96.37%\n",
            "Precision: 97.28%\n",
            "Recall: 95.39999999999999%\n",
            "\n",
            "Important Words: 40\n",
            "Accuracy: 96.43%\n",
            "Precision: 97.41%\n",
            "Recall: 95.39999999999999%\n",
            "\n",
            "Important Words: 50\n",
            "Accuracy: 96.47%\n",
            "Precision: 97.41%\n",
            "Recall: 95.47%\n",
            "\n",
            "Important Words: 100\n",
            "Accuracy: 96.37%\n",
            "Precision: 97.41%\n",
            "Recall: 95.27%\n",
            "\n",
            "Important Words: 200\n",
            "Accuracy: 96.37%\n",
            "Precision: 97.41%\n",
            "Recall: 95.27%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment C, SVM"
      ],
      "metadata": {
        "id": "B2gVTTpgnxnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import sklearn\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "train = pd.read_csv('train_data.csv')\n",
        "test = pd.read_csv('test_data.csv')\n",
        "\n",
        "\n",
        "#token and data cleaning function\n",
        "def clean_text(text):\n",
        "    #removes body prefix\n",
        "    if type(text) != str:\n",
        "        return 'gibberishnonsensenothingeverseenbefore'\n",
        "    #replaces b's at beginning of body paragraphs\n",
        "    if text[:2] == \"b'\" or text[:2] == 'b\"':\n",
        "        text = text[2:]\n",
        "        text = text[:-1]\n",
        "    #replaces digit with digit tags\n",
        "    #text = re.sub(r\"\\d\", \"<digit>\", text)\n",
        "    #removes new line symbols\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "    #removes periods, commas, dashes, apostrophes, quotes, and new lines\n",
        "    text = re.sub(r\"[.,:'/\\-(){}[\\]\\\"]\", \"\", text)\n",
        "    text = re.sub(r'\"', \"\", text)\n",
        "    #removes case\n",
        "    text = text.lower()\n",
        "    return text#.split()\n",
        "\n",
        "#deploy function, add to DFs\n",
        "train_cleaned = []\n",
        "for j in range(len(train)):\n",
        "    train_cleaned.append(clean_text(train.iloc[j]['body']))\n",
        "test_cleaned = []\n",
        "for k in range(len(test)):\n",
        "    test_cleaned.append(clean_text(test.iloc[k]['body']))\n",
        "train['clean'] = train_cleaned\n",
        "test['clean'] = test_cleaned\n",
        "\n",
        "#add parts of speech tags\n",
        "train['tagged'] = train['clean'].apply(lambda x: ' '.join([f'{word}_{tag}' for word, tag in pos_tag(word_tokenize(x))]))\n",
        "test['tagged'] = test['clean'].apply(lambda x: ' '.join([f'{word}_{tag}' for word, tag in pos_tag(word_tokenize(x))]))\n",
        "\n",
        "#vectorize data\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "train_X = vectorizer.fit_transform(train['tagged'])\n",
        "test_X = vectorizer.transform(test['tagged'])\n",
        "\n",
        "tr_binary_labels = []\n",
        "te_binary_labels = []\n",
        "for l in range(len(train)):\n",
        "    if train.iloc[l]['label'] == 'ham':\n",
        "        tr_binary_labels.append(0)\n",
        "    else: tr_binary_labels.append(1)\n",
        "for m in range(len(test)):\n",
        "    if test.iloc[m]['label'] == 'ham':\n",
        "        te_binary_labels.append(0)\n",
        "    else: te_binary_labels.append(1)\n",
        "train_Y = tr_binary_labels\n",
        "test_Y = te_binary_labels\n",
        "\n",
        "\n",
        "#Naive Bayes Model\n",
        "model = LinearSVC()\n",
        "\n",
        "fit = model.fit(train_X, train_Y)\n",
        "\n",
        "predictions = fit.predict(test_X)\n",
        "\n",
        "accuracy = accuracy_score(test_Y, predictions)\n",
        "precision = precision_score(test_Y, predictions)\n",
        "recall = recall_score(test_Y, predictions)\n",
        "\n",
        "print(f'Accuracy: {100*round(accuracy,4)}%')\n",
        "print(f'Precision: {100*round(precision,4)}%')\n",
        "print(f'Recall: {100*round(recall,4)}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFWrKmtan2vO",
        "outputId": "6350d470-2ca0-4c07-deee-31a6faf55d36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 80.23%\n",
            "Precision: 88.14%\n",
            "Recall: 69.87%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment C, Naive Bayes"
      ],
      "metadata": {
        "id": "Y4IIIrLSnwlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import sklearn\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "train = pd.read_csv('train_data.csv')\n",
        "test = pd.read_csv('test_data.csv')\n",
        "\n",
        "\n",
        "#token and data cleaning function\n",
        "def clean_text(text):\n",
        "    #removes body prefix\n",
        "    if type(text) != str:\n",
        "        return 'gibberishnonsensenothingeverseenbefore'\n",
        "    #replaces b's at beginning of body paragraphs\n",
        "    if text[:2] == \"b'\" or text[:2] == 'b\"':\n",
        "        text = text[2:]\n",
        "        text = text[:-1]\n",
        "    #replaces digit with digit tags\n",
        "    #text = re.sub(r\"\\d\", \"<digit>\", text)\n",
        "    #removes new line symbols\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "    #removes periods, commas, dashes, apostrophes, quotes, and new lines\n",
        "    text = re.sub(r\"[.,:'/\\-(){}[\\]\\\"]\", \"\", text)\n",
        "    text = re.sub(r'\"', \"\", text)\n",
        "    #removes case\n",
        "    text = text.lower()\n",
        "    return text#.split()\n",
        "\n",
        "#deploy function, add to DFs\n",
        "train_cleaned = []\n",
        "for j in range(len(train)):\n",
        "    train_cleaned.append(clean_text(train.iloc[j]['body']))\n",
        "test_cleaned = []\n",
        "for k in range(len(test)):\n",
        "    test_cleaned.append(clean_text(test.iloc[k]['body']))\n",
        "train['clean'] = train_cleaned\n",
        "test['clean'] = test_cleaned\n",
        "\n",
        "#add parts of speech tags\n",
        "train['tagged'] = train['clean'].apply(lambda x: ' '.join([f'{word}_{tag}' for word, tag in pos_tag(word_tokenize(x))]))\n",
        "test['tagged'] = test['clean'].apply(lambda x: ' '.join([f'{word}_{tag}' for word, tag in pos_tag(word_tokenize(x))]))\n",
        "\n",
        "#vectorize data\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "train_X = vectorizer.fit_transform(train['tagged'])\n",
        "test_X = vectorizer.transform(test['tagged'])\n",
        "\n",
        "tr_binary_labels = []\n",
        "te_binary_labels = []\n",
        "for l in range(len(train)):\n",
        "    if train.iloc[l]['label'] == 'ham':\n",
        "        tr_binary_labels.append(0)\n",
        "    else: tr_binary_labels.append(1)\n",
        "for m in range(len(test)):\n",
        "    if test.iloc[m]['label'] == 'ham':\n",
        "        te_binary_labels.append(0)\n",
        "    else: te_binary_labels.append(1)\n",
        "train_Y = tr_binary_labels\n",
        "test_Y = te_binary_labels\n",
        "\n",
        "\n",
        "#Naive Bayes Model\n",
        "model = MultinomialNB()\n",
        "\n",
        "fit = model.fit(train_X, train_Y)\n",
        "\n",
        "predictions = fit.predict(test_X)\n",
        "\n",
        "accuracy = accuracy_score(test_Y, predictions)\n",
        "precision = precision_score(test_Y, predictions)\n",
        "recall = recall_score(test_Y, predictions)\n",
        "\n",
        "print(f'Accuracy: {100*round(accuracy,4)}%')\n",
        "print(f'Precision: {100*round(precision,4)}%')\n",
        "print(f'Recall: {100*round(recall,4)}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pV1CJXbn3Nb",
        "outputId": "99249e73-52a5-47c2-9f61-c292b266551b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 69.07%\n",
            "Precision: 84.46000000000001%\n",
            "Recall: 46.73%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}